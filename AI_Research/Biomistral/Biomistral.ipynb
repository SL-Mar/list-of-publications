{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87eef8cd",
   "metadata": {},
   "source": [
    "# An Evaluation of Medical Large Language Models – September 7, 2025 - version 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1ee710",
   "metadata": {},
   "source": [
    "## 0. Key Libraries in the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d255b9",
   "metadata": {},
   "source": [
    "\n",
    "- **llama_cpp_python**: Core library for running quantized GGUF large language models locally with fast inference on CPU and GPU.\n",
    "\n",
    "- **huggingface-hub**: Provides tools to download and cache models from the Hugging Face Hub, enabling seamless model access.\n",
    "\n",
    "- **torch**: PyTorch, a deep learning framework often required by auxiliary tooling or conversion scripts.\n",
    "\n",
    "- **bitsandbytes**: Efficient 8-bit optimizers and quantization utilities, helpful for memory-efficient model handling.\n",
    "\n",
    "- **accelerate**: Hugging Face library to streamline distributed model training and inference.\n",
    "\n",
    "- **numpy**: Fundamental package for numerical computations and array operations.\n",
    "\n",
    "- **ipython / ipykernel / jupyter_client**: Interactive environment and kernel support for running and managing Jupyter notebooks.\n",
    "\n",
    "- **requests**: For HTTP requests, used by huggingface-hub and other networking needs.\n",
    "\n",
    "- **nvidia-cuda-*** libraries: Various CUDA toolkit components for GPU acceleration, essential for offloading inference workloads.\n",
    "\n",
    "- **tqdm**: Progress bar utility for monitoring long-running processes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da816ce8",
   "metadata": {},
   "source": [
    "### Machine Hardware Characteristics\n",
    "\n",
    "The machine running this notebook has the following key CPU and GPU specifications:\n",
    "\n",
    "- **Architecture:** x86_64 (64-bit architecture supporting both 32-bit and 64-bit operation modes)\n",
    "- **Addressable Memory:** 39-bit physical addressing, 48-bit virtual addressing\n",
    "- **CPU Details:**\n",
    "  - Total CPUs: 32 cores (list: 0-31)\n",
    "  - Vendor: Intel (GenuineIntel)\n",
    "  - Model: Intel Core i9-14900HX\n",
    "    - 24 cores and 32 threads\n",
    "    - Hybrid architecture with high-performance cores and efficient cores\n",
    "    - Max turbo frequency up to 5.8 GHz\n",
    "    - Supports DDR5/DDR4 RAM, up to 192 GB memory capacity\n",
    "    - Built on an advanced 10 nm process technology\n",
    "\n",
    "- **GPU Details:**\n",
    "  - Model: NVIDIA GeForce RTX 4080 (Laptop GPU variant)\n",
    "  - Driver Version: 575.64.03, CUDA 12.9\n",
    "  - GPU Temperature: 39°C\n",
    "  - Power Usage: 16W out of 90W capacity\n",
    "  - Memory: 12,282 MiB total, 15 MiB currently used\n",
    "  - GPU Utilization: 0% (idle state)\n",
    "  - Processes using GPU: Xorg display server consuming 4 MiB GPU memory\n",
    "\n",
    "This setup provides a powerful combination of a cutting-edge multi-core CPU and a high-end NVIDIA GPU suitable for intensive computing tasks such as deep learning model inference, especially with large language models that leverage GPU acceleration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bfa916",
   "metadata": {},
   "source": [
    "## 1. Testing Biomistral-7B "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e4c43a",
   "metadata": {},
   "source": [
    "**BioMistral-7B.Q4_K_M.gguf Model Summary**\n",
    "\n",
    "- 7.24 billion parameter LLaMA-based biomedical language model.\n",
    "- Uses efficient 4-bit quantization (Q4_K_M) in GGUF format for fast local inference.\n",
    "- Supports very long context windows up to 32,768 tokens.\n",
    "- Compatible with llama.cpp and runs on consumer CPUs and GPUs.\n",
    "- Excels at generating coherent biomedical text, chat, and summarization.\n",
    "- Reduced memory use thanks to quantization, enabling wider hardware support.\n",
    "- Some accuracy loss due to quantization trade-offs.\n",
    "- Large contexts require significant RAM and GPU power.\n",
    "- Static knowledge cutoff, no real-time updates or internet access.\n",
    "- May generate hallucinated or incorrect outputs; human oversight needed.\n",
    "- Best suited for biomedical NLP research, prototyping, and assistive tasks, not autonomous clinical decision-making.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d5b059",
   "metadata": {},
   "source": [
    "This code imports the Llama class from the llama_cpp library and initializes a local large language model instance by loading the BioMistral-7B.Q4_K_M.gguf model file from the specified path for inference and generation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16b2d24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ./biomistral_model/BioMistral-7B.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = hub\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: printing all EOG tokens:\n",
      "load:   - 2 ('</s>')\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = hub\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 98 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors:   CPU_REPACK model buffer size =  3204.00 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      "repack: repack tensor blk.0.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_v.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.27.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.29.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.29.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.30.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.30.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.31.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.31.ffn_up.weight with q4_K_8x8\n",
      "...................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 512\n",
      "llama_context: n_ctx_per_seq = 512\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (512) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 512 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =    64.00 MiB\n",
      "llama_kv_cache_unified: size =   64.00 MiB (   512 cells,  32 layers,  1/1 seqs), K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 2328\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:        CPU compute buffer size =   113.01 MiB\n",
      "llama_context: graph nodes  = 1126\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token + ' ' }}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'hub', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "model_path = \"./biomistral_model/BioMistral-7B.Q4_K_M.gguf\"\n",
    "llm = Llama(\n",
    "    model_path=\"./biomistral_model/BioMistral-7B.Q4_K_M.gguf\",\n",
    "    n_ctx=512,\n",
    "    n_threads=8,\n",
    "    n_gpu_layers=20  # Offloads 20 layers to GPU for acceleration\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348a18db",
   "metadata": {},
   "source": [
    "This code sends the prompt `\"Explain why rapomycine is effective in treatment of depression\"` to the loaded large language model `llm`, requesting up to 300 tokens in response. It then extracts the generated text from the output, removes any leading or trailing whitespace, and prints the clean, human-readable response prefixed by \"Response:\" for clear display. This process demonstrates how to interact programmatically with the model and retrieve coherent textual answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "447a9d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     347.46 ms\n",
      "llama_perf_context_print: prompt eval time =     347.36 ms /    14 tokens (   24.81 ms per token,    40.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6607.62 ms /    59 runs   (  111.99 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    6970.32 ms /    73 tokens\n",
      "llama_perf_context_print:    graphs reused =         56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      ": Rapamycin is effective in treatment of depression because it inhibits the mTOR pathway. Inhibition of mTOR pathway leads to reduction in neuroinflammation and increases in neurogenesis. This neuroprotective effect may be effective in treatment of depression.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain why rapomycine is effective in treatment of depression\"\n",
    "\n",
    "output = llm(prompt, max_tokens=300)\n",
    "\n",
    "# Extract and clean the generated text\n",
    "response_text = output['choices'][0]['text'].strip()\n",
    "\n",
    "print(\"Response:\\n\" + response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "efa2dada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     347.46 ms\n",
      "llama_perf_context_print: prompt eval time =     412.38 ms /    19 tokens (   21.70 ms per token,    46.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4575.46 ms /    39 runs   (  117.32 ms per token,     8.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    4997.77 ms /    58 tokens\n",
      "llama_perf_context_print:    graphs reused =         37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "A: ECG, Troponin, EchocardiographyB: ECG, Troponin, CT Chest C: ECG, Troponin, Cardiac MRI\n"
     ]
    }
   ],
   "source": [
    "prompt = \"A patient reports chest pain radiating to the left arm. What workup should be done?\"\n",
    "\n",
    "output = llm(prompt, max_tokens=300)\n",
    "\n",
    "# Extract and clean the generated text\n",
    "response_text = output['choices'][0]['text'].strip()\n",
    "\n",
    "print(\"Response:\\n\" + response_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3e09c9",
   "metadata": {},
   "source": [
    "## 2. Testing Bio-Medical-3B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c7e1a9",
   "metadata": {},
   "source": [
    "**Bio-Medical-3B-CoT-012025-GGUF Model Summary**\n",
    "\n",
    "- 3 billion parameter language model based on Qwen2.5-3b-Instruct fine-tuned for biomedical tasks.\n",
    "- Incorporates Chain-of-Thought (CoT) prompting to enhance logical reasoning and step-by-step interpretability.\n",
    "- Trained on a high-quality mixture of over 600,000 synthetic and real biomedical samples, ensuring broad domain coverage.\n",
    "- Designed to generate context-aware, reasoning-driven biomedical text suited for complex question answering and clinical summarization.\n",
    "- Excels at tasks like hypothesis generation, biomedical data interpretation, and supporting evidence-based clinical decisions.\n",
    "- Available in efficient GGUF quantized format optimized for local inference with llama.cpp on CPUs and GPUs.\n",
    "- Balances computational efficiency with advanced reasoning capabilities suitable for research and clinical support.\n",
    "- Has a static knowledge cutoff and requires human supervision to mitigate hallucinations or inaccurate outputs.\n",
    "- Intended as an assistive tool for healthcare professionals, researchers, and educators—not for fully autonomous clinical use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4d95c6",
   "metadata": {},
   "source": [
    "This code snippet demonstrates downloading a model file from Hugging Face Hub and running inference locally using the llama.cpp Python bindings:\n",
    "\n",
    "- `hf_hub_download` fetches the specified model file (`bio-medical-3b-cot-012025-q4_0.gguf`) from the Hugging Face repository `matrixportalx/Bio-Medical-3B-CoT-012025-GGUF`. It handles caching and efficient downloads, returning the local file path.\n",
    "- The `Llama` class loads the model from this downloaded file, configuring:\n",
    "  - `n_ctx=2048` to allow a 2048-token context window for processing longer prompts or documents.\n",
    "  - `n_threads=8` to utilize 8 CPU threads for faster computation.\n",
    "  - `n_gpu_layers=-1` to offload all possible model layers to the GPU for accelerated inference.\n",
    "  - `verbose=True` to print detailed runtime logs including GPU usage.\n",
    "- A medical prompt about chest pain is passed to the model to generate a detailed response with up to 500 tokens, controlling randomness via `temperature=0.7` and nucleus sampling with `top_p=0.9`.\n",
    "- Finally, the generated answer is extracted from the response object and printed after trimming whitespace, providing a clean and concise output.\n",
    "\n",
    "This approach enables efficient local usage of large biomedical language models downloaded securely and managed with Hugging Face tooling while benefiting from GPU acceleration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "448ddeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 38 key-value pairs and 434 tensors from /home/slmar/.cache/huggingface/hub/models--matrixportalx--Bio-Medical-3B-CoT-012025-GGUF/snapshots/90191c301dfbe946f14ef47e1375860d8440f6c8/bio-medical-3b-cot-012025-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 3B Instruct\n",
      "llama_model_loader: - kv   3:                            general.version str              = 012025\n",
      "llama_model_loader: - kv   4:                       general.organization str              = Qwen\n",
      "llama_model_loader: - kv   5:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   6:                           general.basename str              = Qwen2.5\n",
      "llama_model_loader: - kv   7:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   8:                            general.license str              = other\n",
      "llama_model_loader: - kv   9:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv  10:                  general.base_model.0.name str              = Qwen2.5 3b Instruct\n",
      "llama_model_loader: - kv  11:          general.base_model.0.organization str              = Qwen\n",
      "llama_model_loader: - kv  12:              general.base_model.0.repo_url str              = https://huggingface.co/qwen/Qwen2.5-3...\n",
      "llama_model_loader: - kv  13:                      general.dataset.count u32              = 1\n",
      "llama_model_loader: - kv  14:                     general.dataset.0.name str              = BioMedData\n",
      "llama_model_loader: - kv  15:             general.dataset.0.organization str              = Collaiborateorg\n",
      "llama_model_loader: - kv  16:                 general.dataset.0.repo_url str              = https://huggingface.co/collaiborateor...\n",
      "llama_model_loader: - kv  17:                               general.tags arr[str,5]       = [\"generated_from_trainer\", \"medical\",...\n",
      "llama_model_loader: - kv  18:                          qwen2.block_count u32              = 36\n",
      "llama_model_loader: - kv  19:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv  20:                     qwen2.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv  21:                  qwen2.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv  22:                 qwen2.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv  23:              qwen2.attention.head_count_kv u32              = 2\n",
      "llama_model_loader: - kv  24:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  25:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,151667]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,151667]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  31:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  32:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  33:                tokenizer.ggml.bos_token_id u32              = 151644\n",
      "llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  35:                    tokenizer.chat_template str              = {% set system_message = 'You are a he...\n",
      "llama_model_loader: - kv  36:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  37:                          general.file_type u32              = 2\n",
      "llama_model_loader: - type  f32:  181 tensors\n",
      "llama_model_loader: - type q4_0:  252 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_0\n",
      "print_info: file size   = 1.69 GiB (4.71 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 151666 '</thinking>' is not marked as EOG\n",
      "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
      "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
      "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
      "load: control token: 151648 '<|box_start|>' is not marked as EOG\n",
      "load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
      "load: control token: 151649 '<|box_end|>' is not marked as EOG\n",
      "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
      "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
      "load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
      "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
      "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
      "load: control token: 151665 '<thinking>' is not marked as EOG\n",
      "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
      "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
      "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
      "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
      "load: printing all EOG tokens:\n",
      "load:   - 151643 ('<|endoftext|>')\n",
      "load:   - 151645 ('<|im_end|>')\n",
      "load:   - 151662 ('<|fim_pad|>')\n",
      "load:   - 151663 ('<|repo_name|>')\n",
      "load:   - 151664 ('<|file_sep|>')\n",
      "load: special tokens cache size = 24\n",
      "load: token to piece cache size = 0.9311 MB\n",
      "print_info: arch             = qwen2\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 2048\n",
      "print_info: n_layer          = 36\n",
      "print_info: n_head           = 16\n",
      "print_info: n_head_kv        = 2\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 8\n",
      "print_info: n_embd_k_gqa     = 256\n",
      "print_info: n_embd_v_gqa     = 256\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 11008\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = -1\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 3B\n",
      "print_info: model params     = 3.09 B\n",
      "print_info: general.name     = Qwen2.5 3B Instruct\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 151667\n",
      "print_info: n_merges         = 151387\n",
      "print_info: BOS token        = 151644 '<|im_start|>'\n",
      "print_info: EOS token        = 151645 '<|im_end|>'\n",
      "print_info: EOT token        = 151645 '<|im_end|>'\n",
      "print_info: PAD token        = 151643 '<|endoftext|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
      "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "print_info: EOG token        = 151643 '<|endoftext|>'\n",
      "print_info: EOG token        = 151645 '<|im_end|>'\n",
      "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
      "print_info: EOG token        = 151663 '<|repo_name|>'\n",
      "print_info: EOG token        = 151664 '<|file_sep|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  33 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  34 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  35 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  36 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q6_K) (and 182 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors:   CPU_REPACK model buffer size =  1488.38 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  1720.20 MiB\n",
      "repack: repack tensor blk.0.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.0.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.0.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.0.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.0.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.0.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.1.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.1.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.1.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.1.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.1.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.1.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.1.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.2.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.2.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.2.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.2.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.2.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.2.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.2.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.3.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.3.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.3.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.3.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.3.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.3.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.3.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.4.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.4.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.4.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.4.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.4.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.4.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.4.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.5.attn_q.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.5.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.5.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.5.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.5.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.5.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.5.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.6.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.6.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.6.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.6.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.6.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.6.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.6.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.7.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.7.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.7.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.7.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.7.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.7.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.7.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.8.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.8.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.8.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.8.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.8.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.8.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.8.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.9.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.9.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.9.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.9.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.9.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.9.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.9.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.10.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.10.attn_k.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.10.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.10.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.10.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.10.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.10.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.11.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.11.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.11.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.11.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.11.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.11.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.11.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.12.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.12.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.12.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.12.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.12.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.12.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.12.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.13.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.13.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.13.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.13.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.13.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.13.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.13.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.14.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.14.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.14.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.14.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.14.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.14.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.14.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.15.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.15.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.15.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.15.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.15.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.15.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.15.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.16.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.16.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.16.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.16.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.16.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.16.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.16.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.17.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.17.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.17.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.17.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.17.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.17.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.17.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.18.attn_q.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.18.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.18.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.18.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.18.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.18.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.18.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.19.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.19.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.19.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.19.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.19.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.19.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.19.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.20.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.20.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.20.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.20.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.20.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.20.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.20.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.21.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.21.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.21.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.21.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.21.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.21.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.21.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.22.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.22.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.22.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.22.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.22.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.22.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.22.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.23.attn_q.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.23.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.23.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.23.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.23.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.23.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.23.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.24.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.24.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.24.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.24.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.24.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.24.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.24.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.25.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.25.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.25.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.25.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.25.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.25.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.25.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.26.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.26.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.26.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.26.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.26.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.26.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.26.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.27.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.27.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.27.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.27.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.27.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.27.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.27.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.28.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.28.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.28.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.28.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.28.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.28.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.28.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.29.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.29.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.29.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.29.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.29.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.29.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.29.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.30.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.30.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.30.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.30.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.30.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.30.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.30.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.31.attn_q.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.31.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.31.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.31.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.31.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.31.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.31.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.32.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.32.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.32.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.32.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.32.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.32.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.32.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.33.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.33.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.33.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.33.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.33.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.33.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.33.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.34.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.34.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.34.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.34.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.34.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.34.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.34.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.35.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.35.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.35.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.35.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.35.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.35.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.35.ffn_up.weight with q4_0_8x8\n",
      "..\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 2048\n",
      "llama_context: n_ctx_per_seq = 2048\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 1000000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.58 MiB\n",
      "create_memory: n_ctx = 2048 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified: layer  32: dev = CPU\n",
      "llama_kv_cache_unified: layer  33: dev = CPU\n",
      "llama_kv_cache_unified: layer  34: dev = CPU\n",
      "llama_kv_cache_unified: layer  35: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =    72.00 MiB\n",
      "llama_kv_cache_unified: size =   72.00 MiB (  2048 cells,  36 layers,  1/1 seqs), K (f16):   36.00 MiB, V (f16):   36.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 3472\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:        CPU compute buffer size =   300.22 MiB\n",
      "llama_context: graph nodes  = 1374\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{% set system_message = 'You are a helpful assistant.' %}{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% endif %}{% if system_message is defined %}{{ '<|im_start|>system\\n' + system_message + '<|im_end|>\\n' }}{% endif %}{% for message in loop_messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\\n' + content + '<|im_end|>\\n<|im_start|>assistant\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|im_end|>' + '\\n' }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_bos_token': 'false', 'general.size_label': '3B', 'general.type': 'model', 'general.file_type': '2', 'general.dataset.0.organization': 'Collaiborateorg', 'general.finetune': 'Instruct', 'general.organization': 'Qwen', 'general.dataset.count': '1', 'general.dataset.0.name': 'BioMedData', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'general.base_model.0.repo_url': 'https://huggingface.co/qwen/Qwen2.5-3b-Instruct', 'general.version': '012025', 'general.base_model.0.name': 'Qwen2.5 3b Instruct', 'tokenizer.ggml.bos_token_id': '151644', 'general.name': 'Qwen2.5 3B Instruct', 'tokenizer.ggml.pre': 'qwen2', 'general.base_model.count': '1', 'general.base_model.0.organization': 'Qwen', 'general.basename': 'Qwen2.5', 'qwen2.embedding_length': '2048', 'tokenizer.ggml.padding_token_id': '151643', 'qwen2.block_count': '36', 'general.architecture': 'qwen2', 'qwen2.context_length': '32768', 'qwen2.feed_forward_length': '11008', 'tokenizer.ggml.model': 'gpt2', 'qwen2.attention.head_count': '16', 'general.license': 'other', 'qwen2.attention.head_count_kv': '2', 'qwen2.rope.freq_base': '1000000.000000', 'general.dataset.0.repo_url': 'https://huggingface.co/collaiborateorg/BioMedData', 'tokenizer.ggml.eos_token_id': '151645'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% set system_message = 'You are a helpful assistant.' %}{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% endif %}{% if system_message is defined %}{{ '<|im_start|>system\n",
      "' + system_message + '<|im_end|>\n",
      "' }}{% endif %}{% for message in loop_messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\n",
      "' + content + '<|im_end|>\n",
      "<|im_start|>assistant\n",
      "' }}{% elif message['role'] == 'assistant' %}{{ content + '<|im_end|>' + '\n",
      "' }}{% endif %}{% endfor %}\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <|im_start|>\n",
      "llama_perf_context_print:        load time =     264.42 ms\n",
      "llama_perf_context_print: prompt eval time =     264.33 ms /    19 tokens (   13.91 ms per token,    71.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =   23153.77 ms /   494 runs   (   46.87 ms per token,    21.34 tokens per second)\n",
      "llama_perf_context_print:       total time =   23912.17 ms /   513 tokens\n",
      "llama_perf_context_print:    graphs reused =        477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is it an ECG or an MRI? I know both are used to evaluate cardiac issues, but which one is more appropriate for this situation?\n",
      "\n",
      "When someone experiences chest pain radiating to the left arm, it's crucial to choose the right diagnostic test to ensure accurate identification of the issue. An ECG is often the first step because it's quick and gives immediate information about heart rhythm and any ischemic changes. It's like having a quick look at the heart without needing too much time or equipment. But, oh, it can't show blood vessels or other structural issues inside the heart.\n",
      "\n",
      "So, if we think about it, we need something more detailed, right? That's where an MRI comes into play. An MRI can actually show the heart's structure and blood vessels in a really detailed way. That's important because it can catch problems like issues in the coronary arteries, which might not show up on an ECG.\n",
      "\n",
      "Alright, let's think about the situation with chest pain radiating to the arm. This could mean a heart attack or something like pericarditis. These are serious conditions that need specific tests to confirm or rule out, and MRIs are great for that.\n",
      "\n",
      "Hmm, so when you need a detailed look at both heart muscle and blood vessels, MRI seems like the perfect choice. Plus, in cases like this where the diagnosis needs to be precise, an MRI gives the information we need. So, for chest pain radiating to the arm, a cardiac MRI is definitely the go-to choice. It's like having a detailed picture of what's going on inside the heart, which is essential for a proper diagnosis.\n",
      "\n",
      "Yeah, MRI it is. It's the best tool for this scenario. It just seems to fit perfectly with the need for detailed assessment of the heart and coronary arteries.\n",
      "\n",
      "So, in conclusion, for someone with chest pain radiating to the arm, a cardiac MRI is the workup to go for. It matches well with the diagnostic needs for these symptoms. Everything lines up with using an MRI for this purpose. Sounds like a good plan!\n",
      "\n",
      "Final Answer: For someone with chest pain radiating to the arm, a cardiac MRI is the most appropriate workup. It provides a detailed evaluation of both the heart muscle and coronary arteries, which is critical for diagnosing conditions such as heart attack or pericarditis. An MRI is a key diagnostic tool for such cases where detailed imaging is necessary for accurate diagnosis.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "\n",
    "model_name = \"matrixportalx/Bio-Medical-3B-CoT-012025-GGUF\"\n",
    "model_file = \"bio-medical-3b-cot-012025-q4_0.gguf\"\n",
    "model_path = hf_hub_download(model_name, filename=model_file)\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=2048,\n",
    "    n_threads=8,\n",
    "    n_gpu_layers=-1,  # -1 means offload all possible layers to GPU\n",
    "    verbose=True      # Show logs including GPU usage info\n",
    ")\n",
    "\n",
    "prompt = \"A patient reports chest pain radiating to the left arm. What workup should be done?\"\n",
    "response = llm(prompt, max_tokens=500, temperature=0.7, top_p=0.9)\n",
    "\n",
    "print(response['choices'][0]['text'].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f6c38b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biomistral-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
